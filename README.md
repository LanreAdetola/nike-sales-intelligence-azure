
# ğŸ·ï¸ Nike Sales Intelligence Dashboard: Azure Edition

[![Python](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/)
[![Azure](https://img.shields.io/badge/azure-cloud-blueviolet.svg)](https://azure.microsoft.com/)
[![Power BI](https://img.shields.io/badge/power--bi-report-yellowgreen.svg)](https://powerbi.microsoft.com/)
[![License](https://img.shields.io/badge/license-MIT-lightgrey.svg)](LICENSE)

## ğŸ“Š From Raw Data to Smart Decisions

---

### ğŸ” Overview

This project transforms messy, real-world Nike retail sales data into actionable business insights using **Azure cloud services**. It demonstrates how Nikeâ€™s Supply Chain Intelligence team can leverage cloud-scale analytics to optimize product flow, balance speed and cost, and improve profitability.

---

### ğŸ¯ Goals
- Build a **clean, reliable data pipeline** using Azure.
- Perform **exploratory and predictive analysis** on sales data.
- Deliver **interactive Power BI dashboards** for decision-makers.
- Explore **ML models** for profit prediction and sales segmentation.

---

### ğŸ§± Architecture
**Azure Services Used:**
- **Azure Data Factory** â€“ Data ingestion
- **Azure Data Lake Storage Gen2** â€“ Raw and cleaned data storage
- **Azure Databricks** â€“ Data cleaning, EDA, ML modeling
- **Azure SQL / Synapse Analytics** â€“ Structured querying
- **Azure Machine Learning** â€“ Optional ML deployment
- **Power BI** â€“ Visualization
- **GitHub** â€“ Version control and collaboration

---

### ğŸ“ Repository Structure

```
nike-sales-intelligence-azure/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ azure-pipeline.yml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â””â”€â”€ sample/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_cleaning.ipynb
â”‚   â”œâ”€â”€ 02_eda.ipynb
â”‚   â”œâ”€â”€ 03_ml_profit_prediction.ipynb
â”‚   â”œâ”€â”€ 04_ml_segmentation.ipynb
â”‚   â”œâ”€â”€ 05_anomaly_detection.ipynb
â”‚   â””â”€â”€ 06_forecasting.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â”œâ”€â”€ clean_data.py
â”‚   â”œâ”€â”€ ml_models.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ dashboard.pbix
â”‚   â”œâ”€â”€ screenshots/
â”‚   â””â”€â”€ dashboard_notes.md
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ insights_report.md
â”‚   â”œâ”€â”€ timeline.md
â”‚   â””â”€â”€ demo_script.md
â”‚
â””â”€â”€ azure/
    â”œâ”€â”€ setup_instructions.md
    â”œâ”€â”€ bicep_templates/
    â””â”€â”€ terraform/
```

---

### ğŸ“Œ Key Features
- **Cloud-scale data pipeline** from ingestion to visualization
- **ML models** for profit prediction and segmentation
- **Interactive dashboards** with filters and KPIs
- **Real-time updates** via Azure SQL/Synapse integration

---

### ğŸƒ Running Notebooks Locally

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/nike-sales-intelligence-azure.git
cd nike-sales-intelligence-azure
```

2. **Create a virtual environment:**
```bash
python -m venv venv
source venv/bin/activate      # Linux/Mac
venv\Scripts\activate         # Windows
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Launch Jupyter Notebook:**
```bash
jupyter notebook
```

5. Open the notebooks in the `notebooks/` folder and run cells sequentially.

> âš ï¸ **Note:** Some notebooks require access to Azure services (Data Lake, Databricks, SQL). You can mock data in `data/sample/` for local testing.

---

### ğŸ“… Timeline
| Week | Activities |
|------|------------|
| 1    | Load raw dataset to Azure, assess quality, define cleaning strategy |
| 2    | Perform full cleaning in Databricks; document results |
| 3    | Conduct EDA and implement ML models |
| 4    | Build Power BI dashboard; connect to Azure SQL/Synapse |
| 5    | Final report, GitHub publishing, demo video |

---

### ğŸ“ˆ Expected Outcomes
- Insights on top products, regions, and discount strategies
- ML-driven profit predictions and segmentation
- Strategic recommendations for Nikeâ€™s sales and supply chain teams
